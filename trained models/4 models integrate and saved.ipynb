{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b984e1c-1832-42b4-b0a6-4dcb35316543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts: 7473\n",
      "Number of labels: 7473\n",
      "Epoch 1/10\n",
      "187/187 [==============================] - 1s 4ms/step - loss: 0.2997 - accuracy: 0.8934\n",
      "Epoch 2/10\n",
      "187/187 [==============================] - 1s 4ms/step - loss: 0.0322 - accuracy: 0.9938\n",
      "Epoch 3/10\n",
      "187/187 [==============================] - 1s 4ms/step - loss: 0.0106 - accuracy: 0.9980\n",
      "Epoch 4/10\n",
      "187/187 [==============================] - 1s 4ms/step - loss: 0.0048 - accuracy: 0.9997\n",
      "Epoch 5/10\n",
      "187/187 [==============================] - 1s 4ms/step - loss: 0.0026 - accuracy: 0.9998\n",
      "Epoch 6/10\n",
      "187/187 [==============================] - 1s 4ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "187/187 [==============================] - 1s 4ms/step - loss: 8.5866e-04 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "187/187 [==============================] - 1s 4ms/step - loss: 6.0653e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "187/187 [==============================] - 1s 4ms/step - loss: 4.7325e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "187/187 [==============================] - 1s 4ms/step - loss: 3.1210e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      "187/187 [==============================] - 10s 36ms/step - loss: 0.1487 - accuracy: 0.9441\n",
      "Epoch 2/10\n",
      "187/187 [==============================] - 7s 36ms/step - loss: 0.0126 - accuracy: 0.9963\n",
      "Epoch 3/10\n",
      "187/187 [==============================] - 7s 36ms/step - loss: 0.0034 - accuracy: 0.9987\n",
      "Epoch 4/10\n",
      "187/187 [==============================] - 7s 36ms/step - loss: 0.0026 - accuracy: 0.9997\n",
      "Epoch 5/10\n",
      "187/187 [==============================] - 7s 36ms/step - loss: 0.0060 - accuracy: 0.9990\n",
      "Epoch 6/10\n",
      "187/187 [==============================] - 7s 36ms/step - loss: 0.0098 - accuracy: 0.9973\n",
      "Epoch 7/10\n",
      "187/187 [==============================] - 7s 36ms/step - loss: 0.0023 - accuracy: 0.9992\n",
      "Epoch 8/10\n",
      "187/187 [==============================] - 7s 36ms/step - loss: 3.2182e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "187/187 [==============================] - 7s 36ms/step - loss: 9.1647e-05 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "187/187 [==============================] - 7s 37ms/step - loss: 3.5672e-05 - accuracy: 1.0000\n",
      "187/187 [==============================] - 0s 1ms/step\n",
      "187/187 [==============================] - 3s 14ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 1s 14ms/step\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the email text:  this is kunal and i want help\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Logistic Regression Prediction: [1]\n",
      "Neural Network Prediction: [[0.8087951]]\n",
      "BiLSTM Prediction: [[0.99984396]]\n",
      "Naive Bayes Prediction: [0]\n",
      "Consensus: The email is predicted to be spam.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, Bidirectional, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define the data folders\n",
    "lr_nn_data_folder = 'C:\\\\Users\\\\kunalrajput\\\\Downloads\\\\SpamFilterMachineLearning-master\\\\SpamFilterMachineLearning-master\\\\data\\\\'\n",
    "nb_nonspam_folders = [\n",
    "    'D:\\\\spam test for pligun\\\\main test\\\\easy_ham',\n",
    "    'D:\\\\spam test for pligun\\\\main test\\\\easy_ham_2',\n",
    "    'D:\\\\spam test for pligun\\\\main test\\\\hard_ham'\n",
    "]\n",
    "nb_spam_folders = [\n",
    "    'D:\\\\spam test for pligun\\\\main test\\\\spam',\n",
    "    'D:\\\\spam test for pligun\\\\main test\\\\spam_2'\n",
    "]\n",
    "\n",
    "# Function to process and clean a single file\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r', encoding='latin-1') as file:\n",
    "        content = file.read()\n",
    "        content = re.sub(r'\\<\\w{1,2}\\>', '', content)\n",
    "        return content\n",
    "\n",
    "# Combine texts and labels from all datasets\n",
    "all_texts = []\n",
    "all_labels = []\n",
    "\n",
    "# Process all files in the LR+NN data folder\n",
    "for root, _, files in os.walk(lr_nn_data_folder):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        all_texts.append(process_file(file_path))\n",
    "        all_labels.append(0 if 'nonspam' in root else 1)  # Non-spam label if folder contains 'nonspam'\n",
    "\n",
    "# Process all files in the Naive Bayes folders\n",
    "for folder in nb_nonspam_folders:\n",
    "    for root, _, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt') or file.endswith('.txt.txt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                all_texts.append(process_file(file_path))\n",
    "                all_labels.append(0)  # Non-spam labelD\n",
    "\n",
    "for folder in nb_spam_folders:\n",
    "    for root, _, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt') or file.endswith('.txt.txt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                all_texts.append(process_file(file_path))\n",
    "                all_labels.append(1)  # Spam label\n",
    "\n",
    "# Check if lengths of texts and labels are equal\n",
    "print(f\"Number of texts: {len(all_texts)}\")\n",
    "print(f\"Number of labels: {len(all_labels)}\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_texts, X_test_texts, y_train, y_test = train_test_split(all_texts, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_texts)\n",
    "X_test_tfidf = vectorizer.transform(X_test_texts)\n",
    "\n",
    "# Save the vectorizer\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as file:\n",
    "    pickle.dump(vectorizer, file)\n",
    "\n",
    "# Create and train the Naive Bayes model\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Save the Naive Bayes model\n",
    "with open('nb_model.pkl', 'wb') as file:\n",
    "    pickle.dump(nb_model, file)\n",
    "\n",
    "# Tokenization and padding for Neural Network and BiLSTM\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train_texts)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_texts)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_texts)\n",
    "\n",
    "max_seq_length = 100  # Maximum length of a sequence\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_seq_length, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "# Save the tokenizer\n",
    "with open('tokenizer.pkl', 'wb') as file:\n",
    "    pickle.dump(tokenizer, file)\n",
    "\n",
    "# Clear any previous TensorFlow/Keras session state\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Create and train the Neural Network model\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Embedding(input_dim=5000, output_dim=64, input_length=max_seq_length))\n",
    "nn_model.add(GlobalAveragePooling1D())\n",
    "nn_model.add(Dense(64, activation='relu'))\n",
    "nn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "nn_model.fit(X_train_pad, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Save the Neural Network model\n",
    "nn_model_json = nn_model.to_json()\n",
    "with open('nn_model.json', 'w') as json_file:\n",
    "    json_file.write(nn_model_json)\n",
    "nn_model.save_weights('nn_model.h5')\n",
    "\n",
    "# Create and train the BiLSTM model\n",
    "bilstm_model = Sequential()\n",
    "bilstm_model.add(Embedding(input_dim=5000, output_dim=64, input_length=max_seq_length))\n",
    "bilstm_model.add(Bidirectional(LSTM(64)))\n",
    "bilstm_model.add(Dense(64, activation='relu'))\n",
    "bilstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "bilstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "bilstm_model.fit(X_train_pad, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Save the BiLSTM model\n",
    "bilstm_model_json = bilstm_model.to_json()\n",
    "with open('bilstm_model.json', 'w') as json_file:\n",
    "    json_file.write(bilstm_model_json)\n",
    "bilstm_model.save_weights('bilstm_model.h5')\n",
    "\n",
    "# Generate meta-features for training the meta-model (Logistic Regression)\n",
    "nn_train_predictions = nn_model.predict(X_train_pad)\n",
    "bilstm_train_predictions = bilstm_model.predict(X_train_pad)\n",
    "\n",
    "X_train_meta = np.hstack((X_train_tfidf.toarray(), nn_train_predictions, bilstm_train_predictions))\n",
    "\n",
    "# Train the Logistic Regression model using the meta-features\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train_meta, y_train)\n",
    "\n",
    "# Save the Logistic Regression model\n",
    "with open('lr_model.pkl', 'wb') as file:\n",
    "    pickle.dump(lr_model, file)\n",
    "\n",
    "# Generate meta-features for the test set\n",
    "nn_test_predictions = nn_model.predict(X_test_pad)\n",
    "bilstm_test_predictions = bilstm_model.predict(X_test_pad)\n",
    "\n",
    "X_test_meta = np.hstack((X_test_tfidf.toarray(), nn_test_predictions, bilstm_test_predictions))\n",
    "\n",
    "# Function to predict if a given email text is spam or not\n",
    "def predict_email(email_text):\n",
    "    # Predict using NB model\n",
    "    user_features_tfidf = vectorizer.transform([email_text])\n",
    "    nb_prediction = nb_model.predict(user_features_tfidf)\n",
    "    \n",
    "    # Predict using NN model\n",
    "    user_seq = tokenizer.texts_to_sequences([email_text])\n",
    "    user_pad = pad_sequences(user_seq, maxlen=max_seq_length, padding='post')\n",
    "    nn_prediction = nn_model.predict(user_pad)\n",
    "    \n",
    "    # Predict using BiLSTM model\n",
    "    bilstm_prediction = bilstm_model.predict(user_pad)\n",
    "    \n",
    "    # Combine predictions into meta-features\n",
    "    user_meta_features = np.hstack((user_features_tfidf.toarray(), nn_prediction, bilstm_prediction))\n",
    "    \n",
    "    # Predict using LR model\n",
    "    lr_prediction = lr_model.predict(user_meta_features)\n",
    "    \n",
    "    # Print individual model predictions\n",
    "    print(\"Logistic Regression Prediction:\", lr_prediction)\n",
    "    print(\"Neural Network Prediction:\", nn_prediction)\n",
    "    print(\"BiLSTM Prediction:\", bilstm_prediction)\n",
    "    print(\"Naive Bayes Prediction:\", nb_prediction)\n",
    "    \n",
    "    # Weighted majority voting for final prediction\n",
    "    lr_weight = 0.4\n",
    "    nn_weight = 0.2\n",
    "    bilstm_weight = 0.2\n",
    "    nb_weight = 0.2\n",
    "    \n",
    "    final_score = (lr_prediction * lr_weight) + (nn_prediction * nn_weight) + (bilstm_prediction * bilstm_weight) + (nb_prediction * nb_weight)\n",
    "    final_prediction = 1 if final_score >= 0.5 else 0\n",
    "    \n",
    "    # Display final prediction\n",
    "    if final_prediction == 1:\n",
    "        print(\"Consensus: The email is predicted to be spam.\")\n",
    "    else:\n",
    "        print(\"Consensus: The email is not predicted to be spam.\")\n",
    "\n",
    "# Ask the user to enter an email\n",
    "email_text = input(\"Enter the email text: \")\n",
    "predict_email(email_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3edf02db-fbc2-4f3f-bb44-d38d3c0064f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts: 7473\n",
      "Number of labels: 7473\n",
      "Epoch 1/10\n",
      "187/187 [==============================] - 1s 4ms/step - loss: 0.2959 - accuracy: 0.8953\n",
      "Epoch 2/10\n",
      "187/187 [==============================] - 1s 4ms/step - loss: 0.0321 - accuracy: 0.9938\n",
      "Epoch 3/10\n",
      "187/187 [==============================] - 1s 4ms/step - loss: 0.0106 - accuracy: 0.9978\n",
      "Epoch 4/10\n",
      "187/187 [==============================] - 1s 4ms/step - loss: 0.0049 - accuracy: 0.9997\n",
      "Epoch 5/10\n",
      "187/187 [==============================] - 1s 4ms/step - loss: 0.0027 - accuracy: 0.9998\n",
      "Epoch 6/10\n",
      "187/187 [==============================] - 1s 4ms/step - loss: 0.0018 - accuracy: 0.9998\n",
      "Epoch 7/10\n",
      "187/187 [==============================] - 1s 4ms/step - loss: 9.4526e-04 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "187/187 [==============================] - 1s 4ms/step - loss: 6.7453e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "187/187 [==============================] - 1s 4ms/step - loss: 5.3087e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "187/187 [==============================] - 1s 4ms/step - loss: 3.5024e-04 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      "187/187 [==============================] - 12s 42ms/step - loss: 0.1494 - accuracy: 0.9420\n",
      "Epoch 2/10\n",
      "187/187 [==============================] - 8s 41ms/step - loss: 0.0226 - accuracy: 0.9931\n",
      "Epoch 3/10\n",
      "187/187 [==============================] - 8s 41ms/step - loss: 0.0109 - accuracy: 0.9975\n",
      "Epoch 4/10\n",
      "187/187 [==============================] - 7s 40ms/step - loss: 0.0025 - accuracy: 0.9993\n",
      "Epoch 5/10\n",
      "187/187 [==============================] - 8s 43ms/step - loss: 5.7234e-04 - accuracy: 0.9998\n",
      "Epoch 6/10\n",
      "187/187 [==============================] - 7s 40ms/step - loss: 6.6395e-05 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "187/187 [==============================] - 7s 39ms/step - loss: 2.9885e-05 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "187/187 [==============================] - 7s 37ms/step - loss: 1.9614e-05 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "187/187 [==============================] - 7s 37ms/step - loss: 1.4125e-05 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "187/187 [==============================] - 7s 37ms/step - loss: 1.0756e-05 - accuracy: 1.0000\n",
      "187/187 [==============================] - 0s 2ms/step\n",
      "187/187 [==============================] - 4s 17ms/step\n",
      "47/47 [==============================] - 0s 2ms/step\n",
      "47/47 [==============================] - 1s 16ms/step\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the email text:  hii\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Logistic Regression Prediction: [0]\n",
      "Neural Network Prediction: [[0.7428211]]\n",
      "BiLSTM Prediction: [[0.14241438]]\n",
      "Naive Bayes Prediction: [0]\n",
      "Consensus: The email is not predicted to be spam.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, Bidirectional, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Define the data folders\n",
    "lr_nn_data_folder = 'C:\\\\Users\\\\kunalrajput\\\\Downloads\\\\SpamFilterMachineLearning-master\\\\SpamFilterMachineLearning-master\\\\data\\\\'\n",
    "nb_nonspam_folders = [\n",
    "    'D:\\\\spam test for pligun\\\\main test\\\\easy_ham',\n",
    "    'D:\\\\spam test for pligun\\\\main test\\\\easy_ham_2',\n",
    "    'D:\\\\spam test for pligun\\\\main test\\\\hard_ham'\n",
    "]\n",
    "nb_spam_folders = [\n",
    "    'D:\\\\spam test for pligun\\\\main test\\\\spam',\n",
    "    'D:\\\\spam test for pligun\\\\main test\\\\spam_2'\n",
    "]\n",
    "\n",
    "# Function to process and clean a single file\n",
    "def process_file(file_path):\n",
    "    with open(file_path, 'r', encoding='latin-1') as file:\n",
    "        content = file.read()\n",
    "        content = re.sub(r'\\<\\w{1,2}\\>', '', content)\n",
    "        return content\n",
    "\n",
    "# Combine texts and labels from all datasets\n",
    "all_texts = []\n",
    "all_labels = []\n",
    "\n",
    "# Process all files in the LR+NN data folder\n",
    "for root, _, files in os.walk(lr_nn_data_folder):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(Aroot, file)\n",
    "        all_texts.append(process_file(file_path))\n",
    "        all_labels.append(0 if 'nonspam' in root else 1)  # Non-spam label if folder contains 'nonspam'\n",
    "\n",
    "# Process all files in the Naive Bayes folders\n",
    "for folder in nb_nonspam_folders:\n",
    "    for root, _, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt') or file.endswith('.txt.txt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                all_texts.append(process_file(file_path))\n",
    "                all_labels.append(0)  # Non-spam label\n",
    "\n",
    "for folder in nb_spam_folders:\n",
    "    for root, _, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt') or file.endswith('.txt.txt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                all_texts.append(process_file(file_path))\n",
    "                all_labels.append(1)  # Spam label\n",
    "\n",
    "# Check if lengths of texts and labels are equal\n",
    "print(f\"Number of texts: {len(all_texts)}\")\n",
    "print(f\"Number of labels: {len(all_labels)}\")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train_texts, X_test_texts, y_train, y_test = train_test_split(all_texts, all_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_texts)\n",
    "X_test_tfidf = vectorizer.transform(X_test_texts)\n",
    "\n",
    "# Save the vectorizer\n",
    "with open('D:\\\\LR+NN+NB\\\\more accurate\\\\tfidf_vectorizer.pkl', 'wb') as file:\n",
    "    pickle.dump(vectorizer, file)\n",
    "\n",
    "# Create and train the Naive Bayes model\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Save the Naive Bayes model\n",
    "with open('D:\\\\LR+NN+NB\\\\more accurate\\\\nb_model.pkl', 'wb') as file:\n",
    "    pickle.dump(nb_model, file)\n",
    "\n",
    "# Tokenization and padding for Neural Network and BiLSTM\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train_texts)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train_texts)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test_texts)\n",
    "\n",
    "max_seq_length = 100  # Maximum length of a sequence\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_seq_length, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "# Save the tokenizer\n",
    "with open('D:\\\\LR+NN+NB\\\\more accurate\\\\tokenizer.pkl', 'wb') as file:\n",
    "    pickle.dump(tokenizer, file)\n",
    "\n",
    "# Clear any previous TensorFlow/Keras session state\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Create and train the Neural Network model\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Embedding(input_dim=5000, output_dim=64, input_length=max_seq_length))\n",
    "nn_model.add(GlobalAveragePooling1D())\n",
    "nn_model.add(Dense(64, activation='relu'))\n",
    "nn_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "nn_model.fit(X_train_pad, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Save the Neural Network model\n",
    "nn_model_json = nn_model.to_json()\n",
    "with open('D:\\\\LR+NN+NB\\\\more accurate\\\\nn_model.json', 'w') as json_file:\n",
    "    json_file.write(nn_model_json)\n",
    "nn_model.save_weights('D:\\\\LR+NN+NB\\\\more accurate\\\\nn_model.h5')\n",
    "\n",
    "# Create and train the BiLSTM model\n",
    "bilstm_model = Sequential()\n",
    "bilstm_model.add(Embedding(input_dim=5000, output_dim=64, input_length=max_seq_length))\n",
    "bilstm_model.add(Bidirectional(LSTM(64)))\n",
    "bilstm_model.add(Dense(64, activation='relu'))\n",
    "bilstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "bilstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "bilstm_model.fit(X_train_pad, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Save the BiLSTM model\n",
    "bilstm_model_json = bilstm_model.to_json()\n",
    "with open('D:\\\\LR+NN+NB\\\\more accurate\\\\bilstm_model.json', 'w') as json_file:\n",
    "    json_file.write(bilstm_model_json)\n",
    "bilstm_model.save_weights('D:\\\\LR+NN+NB\\\\more accurate\\\\bilstm_model.h5')\n",
    "\n",
    "# Generate meta-features for training the meta-model (Logistic Regression)\n",
    "nn_train_predictions = nn_model.predict(X_train_pad)\n",
    "bilstm_train_predictions = bilstm_model.predict(X_train_pad)\n",
    "\n",
    "X_train_meta = np.hstack((X_train_tfidf.toarray(), nn_train_predictions, bilstm_train_predictions))\n",
    "\n",
    "# Train the Logistic Regression model using the meta-features\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train_meta, y_train)\n",
    "\n",
    "# Save the Logistic Regression model\n",
    "with open('D:\\\\LR+NN+NB\\\\more accurate\\\\lr_model.pkl', 'wb') as file:\n",
    "    pickle.dump(lr_model, file)\n",
    "\n",
    "# Generate meta-features for the test set\n",
    "nn_test_predictions = nn_model.predict(X_test_pad)\n",
    "bilstm_test_predictions = bilstm_model.predict(X_test_pad)\n",
    "\n",
    "X_test_meta = np.hstack((X_test_tfidf.toarray(), nn_test_predictions, bilstm_test_predictions))\n",
    "\n",
    "# Function to predict if a given email text is spam or not\n",
    "def predict_email(email_text):\n",
    "    # Predict using NB model\n",
    "    user_features_tfidf = vectorizer.transform([email_text])\n",
    "    nb_prediction = nb_model.predict(user_features_tfidf)\n",
    "    \n",
    "    # Predict using NN model\n",
    "    user_seq = tokenizer.texts_to_sequences([email_text])\n",
    "    user_pad = pad_sequences(user_seq, maxlen=max_seq_length, padding='post')\n",
    "    nn_prediction = nn_model.predict(user_pad)\n",
    "    \n",
    "    # Predict using BiLSTM model\n",
    "    bilstm_prediction = bilstm_model.predict(user_pad)\n",
    "    \n",
    "    # Combine predictions into meta-features\n",
    "    user_meta_features = np.hstack((user_features_tfidf.toarray(), nn_prediction, bilstm_prediction))\n",
    "    \n",
    "    # Predict using LR model\n",
    "    lr_prediction = lr_model.predict(user_meta_features)\n",
    "    \n",
    "    # Print individual model predictions\n",
    "    print(\"Logistic Regression Prediction:\", lr_prediction)\n",
    "    print(\"Neural Network Prediction:\", nn_prediction)\n",
    "    print(\"BiLSTM Prediction:\", bilstm_prediction)\n",
    "    print(\"Naive Bayes Prediction:\", nb_prediction)\n",
    "    \n",
    "    # Weighted majority voting for final prediction\n",
    "    lr_weight = 0.4\n",
    "    nn_weight = 0.2\n",
    "    bilstm_weight = 0.2\n",
    "    nb_weight = 0.2\n",
    "    \n",
    "    final_score = (lr_prediction * lr_weight) + (nn_prediction * nn_weight) + (bilstm_prediction * bilstm_weight) + (nb_prediction * nb_weight)\n",
    "    final_prediction = 1 if final_score >= 0.5 else 0\n",
    "    \n",
    "    # Display final prediction\n",
    "    if final_prediction == 1:\n",
    "        print(\"Consensus: The email is predicted to be spam.\")\n",
    "    else:\n",
    "        print(\"Consensus: The email is not predicted to be spam.\")\n",
    "\n",
    "# Ask the user to enter an email\n",
    "email_text = input(\"Enter the email text: \")\n",
    "predict_email(email_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
